### My Implementations of standard dot-product attention in various degrees of sophistication

#### Included Kernels
- Naive Unfused Attention (Naive Mat Mul Kernel, Online Softmax Kernel): In Progress
- Flash Attention 2, no tensor cores: Not Started
- Flash Attention 2, with tensor cores: Not Started
- Flash Attention 3: Not Started

#### Credits
- Benchmarking code from [siboehm/SGEMM_CUDA](https://github.com/siboehm/SGEMM_CUDA) (which was from [wangzyon/NVIDIA_SGEMM_PRACTICE](https://github.com/wangzyon/NVIDIA_SGEMM_PRACTICE))
